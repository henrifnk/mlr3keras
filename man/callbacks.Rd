% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/keras_callbacks.R
\name{cb_es}
\alias{cb_es}
\alias{cb_lr_scheduler_cosine_anneal}
\alias{cb_lr_scheduler_exponential_decay}
\alias{cb_tensorboard}
\alias{cb_lr_log}
\alias{LogMetrics}
\alias{SetLogLR}
\title{\code{cb_es}: Early stopping callback}
\usage{
cb_es(monitor = "val_loss", patience = 3L)

cb_lr_scheduler_cosine_anneal(T_max = 10, T_mult = 2, eta_min = 0)

cb_lr_scheduler_exponential_decay()

cb_tensorboard()

cb_lr_log()
}
\arguments{
\item{monitor}{\code{\link{character}}\cr
Quantity to be monitored.}

\item{patience}{\code{\link{integer}}\cr
Number of iterations without improvement to wait before stopping.}

\item{T_max}{\code{\link{integer}}\cr
Reset learning rate every T_max epochs.  Default 10.}

\item{T_mult}{\code{\link{integer}}\cr
Multiply T_max by T_mult every T_max iterations. Default 2.}

\item{eta_min}{\code{\link{numeric}}\cr
Minimal learning rate.}
}
\description{
For more information see:
Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983.
}
\details{
Closed form:
\eqn{\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 +
         \cos(\frac{T_{cur}}{T_{max}}\pi))}
}
