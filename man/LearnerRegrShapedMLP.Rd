% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerKerasShapedMLP.R
\name{LearnerRegrShapedMLP}
\alias{LearnerRegrShapedMLP}
\alias{mlr_learners_regr.keras_smlp}
\title{Keras Feed Forward Neural Network for Regression: Shaped MLP}
\format{
\code{\link[R6:R6Class]{R6::R6Class()}} inheriting from \link{LearnerRegrKeras}.
}
\description{
Shaped MLP as used in Zimmer et al. Auto Pytorch Tabular (2020)
and proposed by https://mikkokotila.github.io/slate.

Note, that some additional hyperparameters (e.g. Embeddings) are made available.

Implements 'Search Space 1' from Zimmer et al. Auto Pytorch Tabular (2020)
(https://arxiv.org/abs/2006.13799)

This learner builds and compiles the keras model from the hyperparameters in \code{param_set},
and does not require a supplied and compiled model.

Calls \code{\link[keras:reexports]{keras::fit()}} from package \CRANpkg{keras}.
Layers are set up as follows:
\itemize{
\item The inputs are connected to a \code{layer_dropout}, applying the \code{input_dropout}.
Afterwards, each \code{layer_dense()} is followed by a \code{layer_activation}, and
depending on hyperparameters by a \code{layer_batch_normalization} and or a
\code{layer_dropout} depending on the architecture hyperparameters.
This is repeated \code{length(layer_units)} times, i.e. one
'dense->activation->batchnorm->dropout' block is appended for each \code{layer_unit}.
The last layer is either 'softmax' or 'sigmoid' for classification or
'linear' or 'sigmoid' for regression.
}

Parameters:\cr
Most of the parameters can be obtained from the \code{keras} documentation.
Some exceptions are documented here.
\itemize{
\item \code{use_embedding}: A logical flag, should embeddings be used?
Either uses \code{make_embedding} (if TRUE) or if set to FALSE \code{model.matrix(~. - 1, data)}
to convert factor, logical and ordered factors into numeric features.
\item \code{n_layers}: An integer defining the number of layers of the shaped MLP.
\item \code{n_max}: An integer, defining the (first layer) number of neurons. The number of neurons is halved
after each layer according to formula (1) in https://arxiv.org/abs/2006.13799.
\item \code{initializer}: Weight and bias initializer.\preformatted{"glorot_uniform"  : initializer_glorot_uniform(seed)
"glorot_normal"   : initializer_glorot_normal(seed)
"he_uniform"      : initializer_he_uniform(seed)
"..."             : see `??keras::initializer`
}
\item \code{optimizer}: Some optimizers and their arguments can be found below.\cr
Inherits from \code{tensorflow.python.keras.optimizer_v2}.\preformatted{"sgd"     : optimizer_sgd(lr, momentum, decay = decay),
"rmsprop" : optimizer_rmsprop(lr, rho, decay = decay),
"adagrad" : optimizer_adagrad(lr, decay = decay),
"adam"    : optimizer_adam(lr, beta_1, beta_2, decay = decay),
"nadam"   : optimizer_nadam(lr, beta_1, beta_2, schedule_decay = decay)
}
\item \code{regularizer}: Regularizer for keras layers:\preformatted{"l1"      : regularizer_l1(l = 0.01)
"l2"      : regularizer_l2(l = 0.01)
"l1_l2"   : regularizer_l1_l2(l1 = 0.01, l2 = 0.01)
}
\item \code{class_weights}: needs to be a named list of class-weights
for the different classes numbered from 0 to c-1 (for c classes).\preformatted{Example:
wts = c(0.5, 1)
setNames(as.list(wts), seq_len(length(wts)) - 1)
}
\item \code{callbacks}: A list of keras callbacks.
See \code{?callbacks}.
}
}
\section{Construction}{
\preformatted{LearnerRegrKerasFF$new()
mlr3::mlr_learners$get("regr.kerasff")
mlr3::lrn("regr.kerasff")
}
}

\section{Learner Methods}{


Keras Learners offer several methods for easy access to the
stored models.
\itemize{
\item \code{.$plot()}\cr
Plots the history, i.e. the train-validation loss during training.
\item \code{.$save(file_path)}\cr
Dumps the model to a provided file_path in 'h5' format.
\item \code{.$load_model_from_file(file_path)}\cr
Loads a model saved using \code{saved} back into the learner.
The model needs to be saved separately when the learner is serialized.
In this case, the learner can be restored from this function.
Currently not implemented for 'TabNet'.
\item \code{.$lr_find(task, epochs, lr_min, lr_max, batch_size)}\cr
Employ an implementation of the learning rate finder as popularized by
Jeremy Howard in fast.ai (http://course.fast.ai/) for the learner.
For more info on parameters, see \code{find_lr}.
}
}

\examples{
learner = mlr3::lrn("regr.keras_smlp")
print(learner)

# available parameters:
learner$param_set$ids()
}
\seealso{
\link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}
}
