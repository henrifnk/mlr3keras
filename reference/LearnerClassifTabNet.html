<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Keras TabNet Neural Network for Classification

Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details. — LearnerClassifTabNet • mlr3keras</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Keras TabNet Neural Network for Classification

Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details. — LearnerClassifTabNet" />
<meta property="og:description" content="Keras TabNet Neural Network for Classification
Implementation of &quot;TabNet&quot; from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details." />
<meta name="twitter:card" content="summary" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">mlr3keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/fist_steps.html">first_steps</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/mlr-org/mlr3keras">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Keras TabNet Neural Network for Classification

Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details.</h1>
    <small class="dont-index">Source: <a href='https://github.com/mlr-org/mlr3keras/blob/master/R/LearnerTabNet.R'><code>R/LearnerTabNet.R</code></a></small>
    <div class="hidden name"><code>LearnerClassifTabNet.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Keras TabNet Neural Network for Classification</p>
<p>Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details.</p>
    </div>



    <h2 class="hasAnchor" id="format"><a class="anchor" href="#format"></a>Format</h2>

    <p><code><a href='https://rdrr.io/pkg/R6/man/R6Class.html'>R6::R6Class()</a></code> inheriting from <a href='LearnerClassifKeras.html'>mlr3keras::LearnerClassifKeras</a>.</p>
    <h2 class="hasAnchor" id="construction"><a class="anchor" href="#construction"></a>Construction</h2>

    
<pre>LearnerClassifTabNet$new()
mlr3::mlr_learners$get("classif.tabnet")
mlr3::lrn("classif.tabnet")
</pre>

    <h2 class="hasAnchor" id="hyper-parameter-tuning"><a class="anchor" href="#hyper-parameter-tuning"></a>Hyper Parameter Tuning</h2>

    


<p>Additional Arguments:</p><ul>
<li><p><code>embed_size</code>: Size of embedding for categorical, character and ordered factors.
Defaults to <code><a href='https://rdrr.io/r/base/Extremes.html'>min(600L, round(1.6 * length(levels)^0.56))</a></code>.</p></li>
<li><p><code>stacked</code>: Should a <code>StackedTabNetModel</code> be used instead of a normal <code>TabNetModel</code>? <br /></p></li>
</ul>

    <h2 class="hasAnchor" id="excerpt-from-paper"><a class="anchor" href="#excerpt-from-paper"></a>Excerpt from paper</h2>

    

<p>We consider datasets ranging from 10K to 10M training points, with varying degrees of fitting
difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter
selection: <br /></p><ul>
<li><p>Most datasets yield the best results for Nsteps between 3 and 10. Typically, larger datasets and
more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from
overfitting and yield poor generalization. <br /></p></li>
<li><p>Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off
between performance and complexity. Nd = Na is a reasonable choice for most datasets. A
very high value of Nd and Na may suffer from overfitting and yield poor generalization. <br /></p></li>
<li><p>An optimal choice of \(\gamma\) can have a major role on the overall performance. Typically a larger
Nsteps value favors for a larger \(\gamma\). <br /></p></li>
<li><p>A large batch size is beneficial for performance - if the memory constraints permit, as large
as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically
much smaller than the batch size. <br /></p></li>
<li><p>Initially large learning rate is important, which should be gradually decayed until convergence. <br /></p></li>
</ul>

<p>The R class wraps a python implementation found in <a href='https://github.com/titu1994/tf-TabNet/tree/master/tabnet'>https://github.com/titu1994/tf-TabNet/tree/master/tabnet</a>.</p>
    <h2 class="hasAnchor" id="parameters"><a class="anchor" href="#parameters"></a>Parameters</h2>

    

<ul>
<li><p><code>feature_dim</code> (N_a): Dimensionality of the hidden representation in feature
transformation block. Each layer first maps the representation to a
2*feature_dim-dimensional output and half of it is used to determine the
nonlinearity of the GLU activation where the other half is used as an
input to GLU, and eventually feature_dim-dimensional output is
transferred to the next layer. <br /></p></li>
<li><p><code>output_dim</code> (N_d): Dimensionality of the outputs of each decision step, which is
later mapped to the final classification or regression output. <br /></p></li>
<li><p><code>num_features</code>: The number of input features (i.e the number of columns for
tabular data assuming each feature is represented with 1 dimension). <br /></p></li>
<li><p><code>num_decision_steps</code> (N_steps): Number of sequential decision steps. <br /></p></li>
<li><p><code>relaxation_factor</code> (gamma): Relaxation factor that promotes the reuse of each
feature at different decision steps. When it is 1, a feature is enforced
to be used only at one decision step and as it increases, more <br />
flexibility is provided to use a feature at multiple decision steps.</p></li>
<li><p><code>sparsity_coefficient</code> (lambda_sparse): Strength of the sparsity regularization.
Sparsity may provide a favorable inductive bias for convergence to
higher accuracy for some datasets where most of the input features are redundant. <br /></p></li>
<li><p><code>norm_type</code>: Type of normalization to perform for the model. Can be either
'batch' or 'group'. 'group' is the default. <br /></p></li>
<li><p><code>batch_momentum</code>: Momentum in ghost batch normalization. <br /></p></li>
<li><p><code>virtual_batch_size</code>: Virtual batch size in ghost batch normalization. The
overall batch size should be an integer multiple of virtual_batch_size. <br /></p></li>
<li><p><code>num_groups</code>: Number of groups used for group normalization. The number of groups
should be a divisor of the number of input features (<code>num_features</code>) <br /></p></li>
<li><p><code>epsilon</code>: A small number for numerical stability of the entropy calculations. <br /></p></li>
</ul>

    <h2 class="hasAnchor" id="learner-methods"><a class="anchor" href="#learner-methods"></a>Learner Methods</h2>

    


<p>Keras Learners offer several methods for easy access to the
stored models.</p><ul>
<li><p><code>.$plot()</code><br />
Plots the history, i.e. the train-validation loss during training.</p></li>
<li><p><code>.$save(file_path)</code><br />
Dumps the model to a provided file_path in 'h5' format.</p></li>
<li><p><code>.$load_model_from_file(file_path)</code><br />
Loads a model saved using <code>saved</code> back into the learner.
The model needs to be saved separately when the learner is serialized.
In this case, the learner can be restored from this function.
Currently not implemented for 'TabNet'.</p></li>
</ul>

    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Sercan, A. and Pfister, T. (2019): TabNet. <a href='https://arxiv.org/abs/1908.07442'>https://arxiv.org/abs/1908.07442</a>.</p>
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><a href='https://mlr3misc.mlr-org.com/reference/Dictionary.html'>Dictionary</a> of <a href='https://mlr3.mlr-org.com/reference/Learner.html'>Learners</a>: <a href='https://mlr3.mlr-org.com/reference/mlr_learners.html'>mlr3::mlr_learners</a></p></div>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='no'>learner</span> <span class='kw'>=</span> <span class='kw pkg'>mlr3</span><span class='kw ns'>::</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span>(<span class='st'>"classif.tabnet"</span>)
<span class='fu'><a href='https://rdrr.io/r/base/print.html'>print</a></span>(<span class='no'>learner</span>)</div><div class='output co'>#&gt; &lt;LearnerClassifTabNet:classif.tabnet&gt;
#&gt; * Model: -
#&gt; * Parameters: epochs=100, validation_split=0, batch_size=128,
#&gt;   callbacks=&lt;list&gt;, low_memory=FALSE, embed_size=&lt;NULL&gt;, stacked=FALSE,
#&gt;   batch_momentum=0.98, relaxation_factor=1, sparsity_coefficient=1e-05,
#&gt;   num_decision_steps=2, feature_dim=4, output_dim=4, num_groups=1,
#&gt;   epsilon=1e-05, norm_type=group, virtual_batch_size=&lt;NULL&gt;,
#&gt;   loss=categorical_crossentropy,
#&gt;   optimizer=&lt;keras.optimizer_v2.adam.Adam&gt;, metrics=accuracy
#&gt; * Packages: keras, tensorflow, reticulate
#&gt; * Predict Type: response
#&gt; * Feature types: integer, numeric, factor, logical
#&gt; * Properties: multiclass, twoclass</div><div class='input'>
<span class='co'># available parameters:</span>
<span class='no'>learner</span>$<span class='no'>param_set</span>$<span class='fu'>ids</span>()</div><div class='output co'>#&gt;  [1] "epochs"               "model"                "class_weight"        
#&gt;  [4] "validation_split"     "batch_size"           "callbacks"           
#&gt;  [7] "low_memory"           "verbose"              "embed_size"          
#&gt; [10] "stacked"              "num_layers"           "batch_momentum"      
#&gt; [13] "relaxation_factor"    "sparsity_coefficient" "num_decision_steps"  
#&gt; [16] "feature_dim"          "output_dim"           "num_groups"          
#&gt; [19] "epsilon"              "norm_type"            "virtual_batch_size"  
#&gt; [22] "loss"                 "optimizer"            "metrics"             </div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#format">Format</a></li>
      <li><a href="#construction">Construction</a></li>
      <li><a href="#hyper-parameter-tuning">Hyper Parameter Tuning</a></li>
      <li><a href="#excerpt-from-paper">Excerpt from paper</a></li>
      <li><a href="#parameters">Parameters</a></li>
      <li><a href="#learner-methods">Learner Methods</a></li>
      <li><a href="#references">References</a></li>
      <li><a href="#see-also">See also</a></li>
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Michel Lang, Florian Pfisterer, Jacky Poon.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


